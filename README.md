# Transformers: From Scratch to Understanding

A learning repository exploring the foundational concepts of Large Language Models (LLMs), starting with the original Transformer architecture.

## Topics Covered

- [Tokenization](bpe_tokenizer.py)
- [Positional Embedding](pos_encoding.py)
- [Scaled Dot-Product Attention](attention.py)
- [Multi-Head Attention](attention.py)
- [Transformer Model](model.py)

## Blog

For detailed explanations with visuals, check out the accompanying blog posts:

- [Coming Soon] - <Blog link>

## Resources

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Original Paper
- [Andrej Karpathy's Github](https://github.com/karpathy) - Check minGPT, nanoGPT, minbpe repos
- [Andrej Karpathy's YouTube](https://www.youtube.com/@AndrejKarpathy) - Check videos about GPT tokenizer and GPT model
- [Medium Post by Sumith Madupu](https://medium.com/@sumith.madupu123/understanding-transformer-architecture-using-simple-math-be6c2e1cdcc7) - A great post explaining transformer architecture with simple math.
- [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) - GPT-2 Paper
- [Gemini Pro](https://gemini.google.com/) - Google's Latest LLM 
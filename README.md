# Transformers: From Scratch to Understanding

A learning repository exploring the foundational concepts of Large Language Models (LLMs), starting with the original Transformer architecture.

## Topics Covered

- Tokenization
- Positional Embedding
- Scaled Dot-Product Attention
- Multi-Head Attention
- Transformer Model

## Blog

For detailed explanations with visuals, check out the accompanying blog posts:

- [Coming Soon] - <Blog link>

## Resources

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Original Paper
- [Andrej Karpathy's Github](https://github.com/karpathy) - Check minGPT, nanoGPT, minbpe repos
- [Andrej Karpathy's YouTube](https://www.youtube.com/@AndrejKarpathy) - Check videos about GPT tokenizer and GPT model


import math
import torch
import torch.nn as nn

class SinusoidalPositionalEncoding(nn.Module):
	"""
    Sinusoidal Positional Encoding code from "Attention is All You Need" paper 
    (Generated by Gemini)
	(I will reimplement it myself in future (maybe)) 
	"""
	def __init__(self, d_model, max_len=5000):
		"""
		d_model: The dimension of the embeddings (e.g., 512)
		max_len: The maximum length of a sequence we expect (e.g., 5000)
		"""
		super().__init__()

		# 1. Initialize a matrix of zeros to hold the PE vectors for all positions up to max_len
		pe = torch.zeros(max_len, d_model) # Shape: [max_len, d_model]
		# 2. Create a vector of positions [0, 1, 2, ... max_len-1]
		position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # Shape: [max_len, 1]
		# 3. Calculate the "div_term" (the denominators)
		# This implements: 1 / (10000 ^ (2i / d_model))
		# calculated in log-space for numerical stability
		div_term = torch.exp(
			torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)
		)
		# 4. Apply Sine to even indices (0, 2, 4...)
		# Mathematical logic: pe[pos, 2i] = sin(pos * div_term)
		pe[:, 0::2] = torch.sin(position * div_term)
		# 5. Apply Cosine to odd indices (1, 3, 5...)
		# Mathematical logic: pe[pos, 2i+1] = cos(pos * div_term)
		pe[:, 1::2] = torch.cos(position * div_term)
		# 6. Add a batch dimension at the front [1, max_len, d_model]
		# This allows us to broadcast it across any batch size later
		pe = pe.unsqueeze(0)
		# 7. Register 'pe' as a buffer (It is part of the state_dict, but not a parameter to optimize)
		self.register_buffer('pe', pe)

	def forward(self, x):
		"""
		x: Input embeddings [batch_size, seq_len, d_model]
		"""
		# Add the positional encoding to the input embedding.
		# We slice self.pe because the current batch might be shorter than max_len.
		# x.size(1) is the sequence length of the current batch.
		x = x + self.pe[:, :x.size(1), :]
		return x


class RoPE:
    """
    Rotary Positional Encoding (RoPE) implementation
    """
    def __init__(self):
        pass